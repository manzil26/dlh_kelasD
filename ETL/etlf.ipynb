{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "342e1b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous data found in DW.\n",
      "Incremental Extract & Transform selesai.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "\n",
    "# Koneksi database\n",
    "hostname = \"localhost\"\n",
    "port = 5432\n",
    "username = \"postgres\"\n",
    "password = \"dataEngginer\"\n",
    "source_db = \"adventureworks\"\n",
    "staging_db = \"staggingDB\"\n",
    "\n",
    "source_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{source_db}\")\n",
    "staging_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{staging_db}\")\n",
    "\n",
    "# ----------------- Dapatkan tanggal terakhir order dari data sebelumnya -----------------\n",
    "try:\n",
    "    last_df = pd.read_sql(\"SELECT MAX(orderdate) as last_date FROM star_schema.fact_sales\", staging_engine)\n",
    "    last_order_date = last_df['last_date'][0]\n",
    "    print(f\"Last order date in DW: {last_order_date}\")\n",
    "except:\n",
    "    last_order_date = None\n",
    "    print(\"No previous data found in DW.\")\n",
    "\n",
    "# ----------------- Extract data baru -----------------\n",
    "query_sales = \"\"\"\n",
    "SELECT soh.orderdate, soh.customerid, soh.territoryid, sod.productid,\n",
    "       sod.orderqty, soh.totaldue\n",
    "FROM sales.salesorderdetail sod\n",
    "JOIN sales.salesorderheader soh ON sod.salesorderid = soh.salesorderid\n",
    "{where_clause}\n",
    "\"\"\"\n",
    "where_clause = f\"WHERE soh.orderdate > '{last_order_date}'\" if last_order_date else \"\"\n",
    "df_sales = pd.read_sql(query_sales.format(where_clause=where_clause), source_engine)\n",
    "\n",
    "if df_sales.empty:\n",
    "    print(\"No new data to process.\")\n",
    "    exit()\n",
    "\n",
    "# ----------------- Buat dimensi -----------------\n",
    "\n",
    "# dim_customer\n",
    "query_customer = \"\"\"\n",
    "SELECT c.customerid,\n",
    "       pp.firstname || ' ' || COALESCE(pp.middlename || ' ', '') || pp.lastname AS customername\n",
    "FROM sales.customer c\n",
    "JOIN person.person pp ON c.personid = pp.businessentityid;\n",
    "\"\"\"\n",
    "df_customer = pd.read_sql(query_customer, source_engine)\n",
    "df_customer['customerKey'] = df_customer['customerid']\n",
    "\n",
    "# dim_product\n",
    "query_product = \"\"\"\n",
    "SELECT p.productid, pc.name AS productsubcategory, p.name AS productname\n",
    "FROM production.product p\n",
    "JOIN production.productsubcategory psc ON p.productsubcategoryid = psc.productsubcategoryid\n",
    "JOIN production.productcategory pc ON psc.productcategoryid = pc.productcategoryid;\n",
    "\"\"\"\n",
    "df_product = pd.read_sql(query_product, source_engine)\n",
    "df_product['productKey'] = df_product['productid']\n",
    "\n",
    "# dim_territory\n",
    "query_territory = \"\"\"\n",
    "SELECT st.territoryid, sp.name AS provincename, cr.name AS countryregion\n",
    "FROM sales.salesterritory st\n",
    "JOIN person.stateprovince sp ON st.territoryID = sp.territoryID\n",
    "JOIN person.countryregion cr ON st.countryregioncode = cr.countryregioncode;\n",
    "\"\"\"\n",
    "df_territory = pd.read_sql(query_territory, source_engine)\n",
    "df_territory['territoryKey'] = df_territory['territoryid']\n",
    "\n",
    "# dim_time\n",
    "df_time = df_sales[['orderdate']].drop_duplicates()\n",
    "df_time['year'] = pd.to_datetime(df_time['orderdate']).dt.year\n",
    "df_time['month'] = pd.to_datetime(df_time['orderdate']).dt.month\n",
    "df_time['day'] = pd.to_datetime(df_time['orderdate']).dt.day\n",
    "df_time['timeKey'] = pd.factorize(df_time['orderdate'])[0] + 1\n",
    "\n",
    "# Join surrogate keys\n",
    "df_sales = df_sales.merge(df_customer[['customerid', 'customerKey']], on='customerid')\n",
    "df_sales = df_sales.merge(df_product[['productid', 'productKey']], on='productid')\n",
    "df_sales = df_sales.merge(df_territory[['territoryid', 'territoryKey']], on='territoryid')\n",
    "df_sales = df_sales.merge(df_time[['orderdate', 'timeKey']], on='orderdate')\n",
    "\n",
    "# Agregasi ke fact_sales\n",
    "df_fact = df_sales.groupby(['customerKey', 'productKey', 'territoryKey', 'timeKey']).agg(\n",
    "    totalQuantity=('orderqty', 'sum'),\n",
    "    averageAmount=('totaldue', 'mean'),\n",
    "    totalRevenue=('totaldue', 'sum')\n",
    ").reset_index()\n",
    "df_fact['salesID'] = range(1, len(df_fact) + 1)\n",
    "\n",
    "# Simpan ke staging.star_schema (sementara, nanti diload ke DW)\n",
    "df_customer.to_sql('dim_customer_incremental', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "df_product.to_sql('dim_product_incremental', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "df_territory.to_sql('dim_territory_incremental', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "df_time[['timeKey', 'year', 'month', 'day']].to_sql('dim_time_incremental', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "df_fact.to_sql('fact_sales_incremental', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "\n",
    "print(\"Incremental Extract & Transform selesai.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84263e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous data found in staging DW.\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "(psycopg2.errors.UndefinedColumn) column \"timekey\" does not exist\nLINE 1: SELECT MAX(timeKey) as max_key FROM star_schema.dim_time\n                   ^\nHINT:  Perhaps you meant to reference the column \"dim_time.timeKey\".\n\n[SQL: SELECT MAX(timeKey) as max_key FROM star_schema.dim_time]\n(Background on this error at: https://sqlalche.me/e/20/f405)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUndefinedColumn\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1964\u001b[39m, in \u001b[36mConnection._exec_single_context\u001b[39m\u001b[34m(self, dialect, context, statement, parameters)\u001b[39m\n\u001b[32m   1963\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[32m-> \u001b[39m\u001b[32m1964\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1965\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1966\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1968\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine._has_events:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:945\u001b[39m, in \u001b[36mDefaultDialect.do_execute\u001b[39m\u001b[34m(self, cursor, statement, parameters, context)\u001b[39m\n\u001b[32m    944\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m945\u001b[39m     \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mUndefinedColumn\u001b[39m: column \"timekey\" does not exist\nLINE 1: SELECT MAX(timeKey) as max_key FROM star_schema.dim_time\n                   ^\nHINT:  Perhaps you meant to reference the column \"dim_time.timeKey\".\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mProgrammingError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     80\u001b[39m df_time[\u001b[33m'\u001b[39m\u001b[33mday\u001b[39m\u001b[33m'\u001b[39m] = df_time[\u001b[33m'\u001b[39m\u001b[33morderdate\u001b[39m\u001b[33m'\u001b[39m].dt.day\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Assign surrogate key timeKey - buat incremental, mulai dari max yang ada di staging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m max_timekey_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSELECT MAX(timeKey) as max_key FROM star_schema.dim_time\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstaging_engine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m max_timekey = max_timekey_df[\u001b[33m'\u001b[39m\u001b[33mmax_key\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     85\u001b[39m df_time = df_time.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\sql.py:734\u001b[39m, in \u001b[36mread_sql\u001b[39m\u001b[34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[39m\n\u001b[32m    724\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql.read_table(\n\u001b[32m    725\u001b[39m         sql,\n\u001b[32m    726\u001b[39m         index_col=index_col,\n\u001b[32m   (...)\u001b[39m\u001b[32m    731\u001b[39m         dtype_backend=dtype_backend,\n\u001b[32m    732\u001b[39m     )\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\sql.py:1836\u001b[39m, in \u001b[36mSQLDatabase.read_query\u001b[39m\u001b[34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_query\u001b[39m(\n\u001b[32m   1780\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1781\u001b[39m     sql: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1788\u001b[39m     dtype_backend: DtypeBackend | Literal[\u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1789\u001b[39m ) -> DataFrame | Iterator[DataFrame]:\n\u001b[32m   1790\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1791\u001b[39m \u001b[33;03m    Read SQL query into a DataFrame.\u001b[39;00m\n\u001b[32m   1792\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1834\u001b[39m \n\u001b[32m   1835\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1836\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1837\u001b[39m     columns = result.keys()\n\u001b[32m   1839\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\sql.py:1659\u001b[39m, in \u001b[36mSQLDatabase.execute\u001b[39m\u001b[34m(self, sql, params)\u001b[39m\n\u001b[32m   1657\u001b[39m args = [] \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m [params]\n\u001b[32m   1658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sql, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1659\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_driver_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1660\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.con.execute(sql, *args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1776\u001b[39m, in \u001b[36mConnection.exec_driver_sql\u001b[39m\u001b[34m(self, statement, parameters, execution_options)\u001b[39m\n\u001b[32m   1771\u001b[39m execution_options = \u001b[38;5;28mself\u001b[39m._execution_options.merge_with(\n\u001b[32m   1772\u001b[39m     execution_options\n\u001b[32m   1773\u001b[39m )\n\u001b[32m   1775\u001b[39m dialect = \u001b[38;5;28mself\u001b[39m.dialect\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1778\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_init_statement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1779\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1780\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1781\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1843\u001b[39m, in \u001b[36mConnection._execute_context\u001b[39m\u001b[34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[39m\n\u001b[32m   1841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exec_insertmany_context(dialect, context)\n\u001b[32m   1842\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1843\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_exec_single_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\n\u001b[32m   1845\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1983\u001b[39m, in \u001b[36mConnection._exec_single_context\u001b[39m\u001b[34m(self, dialect, context, statement, parameters)\u001b[39m\n\u001b[32m   1980\u001b[39m     result = context._setup_result_proxy()\n\u001b[32m   1982\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1983\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1984\u001b[39m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1987\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2352\u001b[39m, in \u001b[36mConnection._handle_dbapi_exception\u001b[39m\u001b[34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[39m\n\u001b[32m   2350\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[32m   2351\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2352\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception.with_traceback(exc_info[\u001b[32m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2353\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2354\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[32m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1964\u001b[39m, in \u001b[36mConnection._exec_single_context\u001b[39m\u001b[34m(self, dialect, context, statement, parameters)\u001b[39m\n\u001b[32m   1962\u001b[39m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1963\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[32m-> \u001b[39m\u001b[32m1964\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1965\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1966\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1968\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine._has_events:\n\u001b[32m   1969\u001b[39m     \u001b[38;5;28mself\u001b[39m.dispatch.after_cursor_execute(\n\u001b[32m   1970\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1971\u001b[39m         cursor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1975\u001b[39m         context.executemany,\n\u001b[32m   1976\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:945\u001b[39m, in \u001b[36mDefaultDialect.do_execute\u001b[39m\u001b[34m(self, cursor, statement, parameters, context)\u001b[39m\n\u001b[32m    944\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m945\u001b[39m     \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mProgrammingError\u001b[39m: (psycopg2.errors.UndefinedColumn) column \"timekey\" does not exist\nLINE 1: SELECT MAX(timeKey) as max_key FROM star_schema.dim_time\n                   ^\nHINT:  Perhaps you meant to reference the column \"dim_time.timeKey\".\n\n[SQL: SELECT MAX(timeKey) as max_key FROM star_schema.dim_time]\n(Background on this error at: https://sqlalche.me/e/20/f405)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "# Koneksi database\n",
    "hostname = \"localhost\"\n",
    "port = 5432\n",
    "username = \"postgres\"\n",
    "password = \"dataEngginer\"\n",
    "staging_db = \"staggingDB\"\n",
    "dw_db = \"adventureworksDw\"\n",
    "\n",
    "staging_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{staging_db}\")\n",
    "dw_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{dw_db}\")\n",
    "\n",
    "# Daftar tabel incremental\n",
    "incremental_tables = [\n",
    "    ('dim_customer_incremental', 'dim_customer'),\n",
    "    ('dim_product_incremental', 'dim_product'),\n",
    "    ('dim_territory_incremental', 'dim_territory'),\n",
    "    ('dim_time_incremental', 'dim_time'),\n",
    "    ('fact_sales_incremental', 'fact_sales')\n",
    "]\n",
    "\n",
    "for source_table, target_table in incremental_tables:\n",
    "    try:\n",
    "        df = pd.read_sql(f\"SELECT * FROM star_schema.{source_table}\", staging_engine)\n",
    "        if df.empty:\n",
    "            print(f\"Tidak ada data untuk {target_table}, skip.\")\n",
    "            continue\n",
    "        df.to_sql(target_table, dw_engine, if_exists='append', index=False)\n",
    "        print(f\"{target_table} berhasil di-append.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Gagal memproses tabel {target_table}: {e}\")\n",
    "\n",
    "print(\"Incremental Load selesai.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae3ef700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- Incremental Extract & Transform -------------------\n",
      "Gagal ambil max modifieddate dari DW: (psycopg2.errors.UndefinedTable) relation \"star_schema.dim_product\" does not exist\n",
      "LINE 1: SELECT MAX(modifieddate) FROM star_schema.dim_product\n",
      "                                      ^\n",
      "\n",
      "[SQL: SELECT MAX(modifieddate) FROM star_schema.dim_product]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "Last modified date in Staging: 2025-05-28 20:55:37.791273\n",
      "Jumlah data baru yang ditemukan: 0\n",
      "Tidak ada data baru yang perlu diproses.\n",
      "Incremental Extract & Transform selesai.\n"
     ]
    }
   ],
   "source": [
    "# incremental_etl_extract_transform.py\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------- Koneksi DB -------------------\n",
    "hostname = \"localhost\"\n",
    "port = 5432\n",
    "username = \"postgres\"\n",
    "password = \"dataEngginer\"\n",
    "source_db = \"adventureworks\"\n",
    "staging_db = \"staggingDB\"\n",
    "dw_db = \"adventureworksDw\"\n",
    "\n",
    "source_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{source_db}\")\n",
    "staging_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{staging_db}\")\n",
    "dw_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{dw_db}\")\n",
    "\n",
    "print(\"------------------- Incremental Extract & Transform -------------------\")\n",
    "\n",
    "# Ambil MAX(modifieddate) dari DW dan Staging\n",
    "try:\n",
    "    max_dw = pd.read_sql(\"SELECT MAX(modifieddate) FROM star_schema.dim_product\", dw_engine).iloc[0, 0]\n",
    "    if pd.isnull(max_dw):\n",
    "        max_dw = datetime(2000, 1, 1)\n",
    "    print(f\"Last modified date in DW: {max_dw}\")\n",
    "except Exception as e:\n",
    "    print(f\"Gagal ambil max modifieddate dari DW: {e}\")\n",
    "    max_dw = datetime(2000, 1, 1)\n",
    "\n",
    "try:\n",
    "    max_staging = pd.read_sql(\"SELECT MAX(modifieddate) FROM raw_schema.product\", staging_engine).iloc[0, 0]\n",
    "    if pd.isnull(max_staging):\n",
    "        max_staging = datetime(2000, 1, 1)\n",
    "    print(f\"Last modified date in Staging: {max_staging}\")\n",
    "except Exception as e:\n",
    "    print(f\"Gagal ambil max modifieddate dari Staging: {e}\")\n",
    "    max_staging = datetime(2000, 1, 1)\n",
    "\n",
    "# Ambil data baru dari OLTP\n",
    "query_incremental = f'''\n",
    "    SELECT p.productid, p.name AS productname, p.modifieddate,\n",
    "           psc.name AS productsubcategory, pc.name AS productcategory\n",
    "    FROM production.product p\n",
    "    LEFT JOIN production.productsubcategory psc ON p.productsubcategoryid = psc.productsubcategoryid\n",
    "    LEFT JOIN production.productcategory pc ON psc.productcategoryid = pc.productcategoryid\n",
    "    WHERE p.modifieddate > '{max(max_dw, max_staging)}'\n",
    "'''\n",
    "\n",
    "df_new_products = pd.read_sql(query_incremental, source_engine)\n",
    "print(f\"Jumlah data baru yang ditemukan: {len(df_new_products)}\")\n",
    "\n",
    "if not df_new_products.empty:\n",
    "    df_new_products['productKey'] = range(1, len(df_new_products) + 1)\n",
    "    df_new_products.to_sql('dim_product', staging_engine, schema='star_schema', if_exists='append', index=False)\n",
    "    print(\"Data baru berhasil ditransformasi dan disimpan ke staging star_schema.dim_product.\")\n",
    "else:\n",
    "    print(\"Tidak ada data baru yang perlu diproses.\")\n",
    "\n",
    "print(\"Incremental Extract & Transform selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61095381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- Incremental Load -------------------\n",
      "Gagal load ke DW: (psycopg2.errors.InvalidSchemaName) schema \"star_schema\" does not exist\n",
      "LINE 2: CREATE TABLE star_schema.dim_product (\n",
      "                     ^\n",
      "\n",
      "[SQL: \n",
      "CREATE TABLE star_schema.dim_product (\n",
      "\tproductid BIGINT, \n",
      "\tproductsubcategory TEXT, \n",
      "\tproductname TEXT, \n",
      "\t\"productKey\" BIGINT\n",
      ")\n",
      "\n",
      "]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "Incremental Load selesai.\n"
     ]
    }
   ],
   "source": [
    "# incremental_etl_load.py\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "# ------------------- Koneksi DB -------------------\n",
    "hostname = \"localhost\"\n",
    "port = 5432\n",
    "username = \"postgres\"\n",
    "password = \"dataEngginer\"\n",
    "dw_db = \"adventureworksDw\"\n",
    "staging_db = \"staggingDB\"\n",
    "\n",
    "dw_engine = create_engine(f'postgresql://{username}:{password}@{hostname}:{port}/{dw_db}')\n",
    "staging_engine = create_engine(f'postgresql://{username}:{password}@{hostname}:{port}/{staging_db}')\n",
    "\n",
    "print(\"------------------- Incremental Load -------------------\")\n",
    "\n",
    "try:\n",
    "    # Cek apakah tabel staging ada dan berisi data\n",
    "    df_staging = pd.read_sql(\"SELECT * FROM star_schema.dim_product\", staging_engine)\n",
    "    if df_staging.empty:\n",
    "        print(\"Staging star_schema.dim_product kosong. Tidak ada yang perlu diload.\")\n",
    "    else:\n",
    "        # Load ke DW\n",
    "        inspector = inspect(dw_engine)\n",
    "        if 'dim_product' in inspector.get_table_names(schema='star_schema'):\n",
    "            df_staging.to_sql('dim_product', dw_engine, schema='star_schema', if_exists='append', index=False)\n",
    "            print(f\"{len(df_staging)} baris data berhasil di-load ke DW star_schema.dim_product.\")\n",
    "        else:\n",
    "            df_staging.to_sql('dim_product', dw_engine, schema='star_schema', if_exists='fail', index=False)\n",
    "            print(\"Tabel dim_product baru berhasil dibuat dan data dimuat.\")\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"Gagal load ke DW: {e}\")\n",
    "\n",
    "print(\"Incremental Load selesai.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
