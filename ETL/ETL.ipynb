{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b2081f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- extraction and transformation -------------------\n",
      "------------- inserting raw tables to raw_schema in staging -------------\n",
      "Raw tables successfully inserted into raw_schema.\n",
      "ETL selesai. Data berhasil disimpan ke staging database:\n",
      "- raw_schema: semua tabel sumber \n",
      "- star_schema: tabel dim_customer, dim_product, dim_territory, dim_time, fact_sales\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------- extraction and transformation -------------------\")\n",
    "import pandas as pd  \n",
    "from sqlalchemy import create_engine  \n",
    "\n",
    "# ------------------- Koneksi Database -------------------\n",
    "hostname = \"localhost\"\n",
    "port = 5432\n",
    "username = \"postgres\"\n",
    "password = \"dataEngginer\"\n",
    "source_db = \"adventureworks\"  \n",
    "staging_db = \"staggingDB\"      \n",
    "\n",
    "# Koneksi ke database source dan staging (sudah diperbaiki)\n",
    "source_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{source_db}\")\n",
    "staging_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{staging_db}\")\n",
    "\n",
    "# ------------------- RAW DATA ke raw_schema -------------------\n",
    "print(\"------------- inserting raw tables to raw_schema in staging -------------\")\n",
    "\n",
    "df_raw_customer = pd.read_sql(\"SELECT * FROM sales.customer\", source_engine)\n",
    "df_raw_customer.to_sql('customer', staging_engine, schema='raw_schema', if_exists='replace', index=False)\n",
    "\n",
    "df_raw_person = pd.read_sql(\"SELECT * FROM person.person\", source_engine)\n",
    "df_raw_person.to_sql('person', staging_engine, schema='raw_schema', if_exists='replace', index=False)\n",
    "\n",
    "df_raw_product = pd.read_sql(\"SELECT * FROM production.product\", source_engine)\n",
    "df_raw_product.to_sql('product', staging_engine, schema='raw_schema', if_exists='replace', index=False)\n",
    "\n",
    "df_raw_pc = pd.read_sql(\"SELECT * FROM production.productcategory\", source_engine)\n",
    "df_raw_pc.to_sql('productcategory', staging_engine, schema='raw_schema', if_exists='replace', index=False)\n",
    "\n",
    "df_raw_psc = pd.read_sql(\"SELECT * FROM production.productsubcategory\", source_engine)\n",
    "df_raw_psc.to_sql('productsubcategory', staging_engine, schema='raw_schema', if_exists='replace', index=False)\n",
    "\n",
    "df_raw_soh = pd.read_sql(\"SELECT * FROM sales.salesorderheader\", source_engine)\n",
    "df_raw_soh.to_sql('salesorderheader', staging_engine, schema='raw_schema', if_exists='replace', index=False)\n",
    "\n",
    "df_raw_sod = pd.read_sql(\"SELECT * FROM sales.salesorderdetail\", source_engine)\n",
    "df_raw_sod.to_sql('salesorderdetail', staging_engine, schema='raw_schema', if_exists='replace', index=False)\n",
    "\n",
    "df_raw_st = pd.read_sql(\"SELECT * FROM sales.salesterritory\", source_engine)\n",
    "df_raw_st.to_sql('salesterritory', staging_engine, schema='raw_schema', if_exists='replace', index=False)\n",
    "\n",
    "df_raw_sp = pd.read_sql(\"SELECT * FROM person.stateprovince\", source_engine)\n",
    "df_raw_sp.to_sql('stateprovince', staging_engine, schema='raw_schema', if_exists='replace', index=False)\n",
    "\n",
    "df_raw_cr = pd.read_sql(\"SELECT * FROM person.countryregion\", source_engine)\n",
    "df_raw_cr.to_sql('countryregion', staging_engine, schema='raw_schema', if_exists='replace', index=False)\n",
    "\n",
    "print(\"Raw tables successfully inserted into raw_schema.\")\n",
    "\n",
    "# ------------------- EXTRACT + TRANSFORM -------------------\n",
    "\n",
    "# DIM_CUSTOMER\n",
    "query_customer = \"\"\"\n",
    "SELECT c.customerid,\n",
    "       pp.firstname || ' ' || COALESCE(pp.middlename || ' ', '') || pp.lastname AS customername\n",
    "FROM sales.customer c\n",
    "JOIN person.person pp ON c.personid = pp.businessentityid;\n",
    "\"\"\"\n",
    "df_customer = pd.read_sql(query_customer, source_engine)\n",
    "df_customer['customerKey'] = range(1, len(df_customer)+1)\n",
    "\n",
    "# DIM_PRODUCT\n",
    "query_product = \"\"\"\n",
    "SELECT p.productid, pc.name AS productsubcategory, p.name AS productname\n",
    "FROM production.product p\n",
    "JOIN production.productsubcategory psc ON p.productsubcategoryid = psc.productsubcategoryid\n",
    "JOIN production.productcategory pc ON psc.productcategoryid = pc.productcategoryid;\n",
    "\"\"\"\n",
    "df_product = pd.read_sql(query_product, source_engine)\n",
    "df_product['productKey'] = range(1, len(df_product)+1)\n",
    "\n",
    "# DIM_TERRITORY\n",
    "query_territory = \"\"\"\n",
    "SELECT st.territoryid, sp.name AS provincename, cr.name AS countryregion\n",
    "FROM sales.salesterritory st\n",
    "JOIN person.stateprovince sp ON st.territoryID = sp.territoryID\n",
    "JOIN person.countryregion cr ON st.countryregioncode = cr.countryregioncode;\n",
    "\"\"\"\n",
    "df_territory = pd.read_sql(query_territory, source_engine)\n",
    "df_territory['territoryKey'] = range(1, len(df_territory)+1)\n",
    "\n",
    "# FACT_SALES\n",
    "query_sales = \"\"\"\n",
    "SELECT soh.orderdate, soh.customerid, soh.territoryid, sod.productid,\n",
    "       sod.orderqty, soh.totaldue\n",
    "FROM sales.salesorderdetail sod\n",
    "JOIN sales.salesorderheader soh ON sod.salesorderid = soh.salesorderid;\n",
    "\"\"\"\n",
    "df_sales = pd.read_sql(query_sales, source_engine)\n",
    "\n",
    "# DIM_TIME\n",
    "df_time = df_sales[['orderdate']].drop_duplicates()\n",
    "df_time['year'] = pd.to_datetime(df_time['orderdate']).dt.year\n",
    "df_time['month'] = pd.to_datetime(df_time['orderdate']).dt.month\n",
    "df_time['day'] = pd.to_datetime(df_time['orderdate']).dt.day\n",
    "df_time['timeKey'] = range(1, len(df_time)+1)\n",
    "\n",
    "# Join surrogate keys ke fact_sales\n",
    "df_sales = df_sales.merge(df_customer[['customerid', 'customerKey']], on='customerid', how='left')\n",
    "df_sales = df_sales.merge(df_product[['productid', 'productKey']], on='productid', how='left')\n",
    "df_sales = df_sales.merge(df_territory[['territoryid', 'territoryKey']], on='territoryid', how='left')\n",
    "df_sales = df_sales.merge(df_time[['orderdate', 'timeKey']], on='orderdate', how='left')\n",
    "\n",
    "# Agregasi fact_sales\n",
    "df_fact = df_sales.groupby(['customerKey', 'productKey', 'territoryKey', 'timeKey']).agg(\n",
    "    totalQuantity=('orderqty', 'sum'),\n",
    "    averageAmount=('totaldue', 'mean'),\n",
    "    totalRevenue=('totaldue', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "df_fact['salesID'] = range(1, len(df_fact)+1)\n",
    "\n",
    "# ------------------- LOAD KE star_schema -------------------\n",
    "\n",
    "df_customer.to_sql('dim_customer', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "df_product.to_sql('dim_product', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "df_territory.to_sql('dim_territory', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "df_time[['timeKey', 'year', 'month', 'day']].to_sql('dim_time', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "df_fact.to_sql('fact_sales', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "\n",
    "# ------------------- DONE -------------------\n",
    "print(\"ETL selesai. Data berhasil disimpan ke staging database:\")\n",
    "print(\"- raw_schema: semua tabel sumber \")\n",
    "print(\"- star_schema: tabel dim_customer, dim_product, dim_territory, dim_time, fact_sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a43944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- Load data ke Data Warehouse -------------------\n",
      "Loading table dim_customer...\n",
      "Table dim_customer berhasil di-replace di Data Warehouse.\n",
      "Loading table dim_product...\n",
      "Table dim_product berhasil di-replace di Data Warehouse.\n",
      "Loading table dim_territory...\n",
      "Table dim_territory berhasil di-replace di Data Warehouse.\n",
      "Loading table dim_time...\n",
      "Table dim_time berhasil di-replace di Data Warehouse.\n",
      "Loading table fact_sales...\n",
      "Table fact_sales berhasil di-replace di Data Warehouse.\n",
      "Load ke Data Warehouse selesai.\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------- Load data ke Data Warehouse -------------------\")\n",
    "import pandas as pd  \n",
    "from sqlalchemy import create_engine, inspect\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "# Koneksi database\n",
    "hostname = \"localhost\"  \n",
    "port = 5432  \n",
    "username = \"postgres\"  \n",
    "password = \"dataEngginer\"  \n",
    "dw_db = \"adventureworksDw\"  \n",
    "staging_db = \"staggingDB\"  \n",
    "\n",
    "# Membuat koneksi ke data warehouse menggunakan SQLAlchemy\n",
    "dw_engine = create_engine(f'postgresql://{username}:{password}@{hostname}:{port}/{dw_db}')\n",
    "# Membuat koneksi ke staging database menggunakan SQLAlchemy\n",
    "staging_engine = create_engine(f'postgresql://{username}:{password}@{hostname}:{port}/{staging_db}')\n",
    "\n",
    "# List tabel star schema yang akan di-load\n",
    "tables = ['dim_customer', 'dim_product', 'dim_territory', 'dim_time', 'fact_sales']\n",
    "\n",
    "def load_table(table_name):\n",
    "    try:\n",
    "        print(f\"Loading table {table_name}...\")\n",
    "        # Ambil data dari staging\n",
    "        df = pd.read_sql(f'SELECT * FROM star_schema.{table_name}', staging_engine)\n",
    "\n",
    "        # Cek apakah tabel sudah ada di data warehouse\n",
    "        inspector = inspect(dw_engine)\n",
    "        if table_name in inspector.get_table_names():\n",
    "            # replace supaya data update total\n",
    "            df.to_sql(table_name, dw_engine, if_exists='replace', index=False)\n",
    "            print(f\"Table {table_name} berhasil di-replace di Data Warehouse.\")\n",
    "        else:\n",
    "            df.to_sql(table_name, dw_engine, if_exists='fail', index=False)\n",
    "            print(f\"Table {table_name} berhasil di-load di Data Warehouse.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"Gagal load table {table_name}: {e}\")\n",
    "\n",
    "# Looping load semua tabel\n",
    "for table in tables:\n",
    "    load_table(table)\n",
    "\n",
    "print(\"Load ke Data Warehouse selesai.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "333f53ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous data found in DW.\n",
      "Incremental Extract & Transform selesai.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "\n",
    "# Koneksi database\n",
    "hostname = \"localhost\"\n",
    "port = 5432\n",
    "username = \"postgres\"\n",
    "password = \"dataEngginer\"\n",
    "source_db = \"adventureworks\"\n",
    "staging_db = \"staggingDB\"\n",
    "\n",
    "source_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{source_db}\")\n",
    "staging_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{staging_db}\")\n",
    "\n",
    "# ----------------- Dapatkan tanggal terakhir order dari data sebelumnya -----------------\n",
    "try:\n",
    "    last_df = pd.read_sql(\"SELECT MAX(orderdate) as last_date FROM star_schema.fact_sales\", staging_engine)\n",
    "    last_order_date = last_df['last_date'][0]\n",
    "    print(f\"Last order date in DW: {last_order_date}\")\n",
    "except:\n",
    "    last_order_date = None\n",
    "    print(\"No previous data found in DW.\")\n",
    "\n",
    "# ----------------- Extract data baru -----------------\n",
    "query_sales = \"\"\"\n",
    "SELECT soh.orderdate, soh.customerid, soh.territoryid, sod.productid,\n",
    "       sod.orderqty, soh.totaldue\n",
    "FROM sales.salesorderdetail sod\n",
    "JOIN sales.salesorderheader soh ON sod.salesorderid = soh.salesorderid\n",
    "{where_clause}\n",
    "\"\"\"\n",
    "where_clause = f\"WHERE soh.orderdate > '{last_order_date}'\" if last_order_date else \"\"\n",
    "df_sales = pd.read_sql(query_sales.format(where_clause=where_clause), source_engine)\n",
    "\n",
    "if df_sales.empty:\n",
    "    print(\"No new data to process.\")\n",
    "    exit()\n",
    "\n",
    "# ----------------- dimensi -----------------\n",
    "\n",
    "# dim_customer\n",
    "query_customer = \"\"\"\n",
    "SELECT c.customerid,\n",
    "       pp.firstname || ' ' || COALESCE(pp.middlename || ' ', '') || pp.lastname AS customername\n",
    "FROM sales.customer c\n",
    "JOIN person.person pp ON c.personid = pp.businessentityid;\n",
    "\"\"\"\n",
    "df_customer = pd.read_sql(query_customer, source_engine)\n",
    "df_customer['customerKey'] = df_customer['customerid']\n",
    "\n",
    "# dim_product\n",
    "query_product = \"\"\"\n",
    "SELECT p.productid, pc.name AS productsubcategory, p.name AS productname\n",
    "FROM production.product p\n",
    "JOIN production.productsubcategory psc ON p.productsubcategoryid = psc.productsubcategoryid\n",
    "JOIN production.productcategory pc ON psc.productcategoryid = pc.productcategoryid;\n",
    "\"\"\"\n",
    "df_product = pd.read_sql(query_product, source_engine)\n",
    "df_product['productKey'] = df_product['productid']\n",
    "\n",
    "# dim_territory\n",
    "query_territory = \"\"\"\n",
    "SELECT st.territoryid, sp.name AS provincename, cr.name AS countryregion\n",
    "FROM sales.salesterritory st\n",
    "JOIN person.stateprovince sp ON st.territoryID = sp.territoryID\n",
    "JOIN person.countryregion cr ON st.countryregioncode = cr.countryregioncode;\n",
    "\"\"\"\n",
    "df_territory = pd.read_sql(query_territory, source_engine)\n",
    "df_territory['territoryKey'] = df_territory['territoryid']\n",
    "\n",
    "# dim_time\n",
    "df_time = df_sales[['orderdate']].drop_duplicates()\n",
    "df_time['year'] = pd.to_datetime(df_time['orderdate']).dt.year\n",
    "df_time['month'] = pd.to_datetime(df_time['orderdate']).dt.month\n",
    "df_time['day'] = pd.to_datetime(df_time['orderdate']).dt.day\n",
    "df_time['timeKey'] = pd.factorize(df_time['orderdate'])[0] + 1\n",
    "\n",
    "# Join surrogate keys\n",
    "df_sales = df_sales.merge(df_customer[['customerid', 'customerKey']], on='customerid')\n",
    "df_sales = df_sales.merge(df_product[['productid', 'productKey']], on='productid')\n",
    "df_sales = df_sales.merge(df_territory[['territoryid', 'territoryKey']], on='territoryid')\n",
    "df_sales = df_sales.merge(df_time[['orderdate', 'timeKey']], on='orderdate')\n",
    "\n",
    "# Agregasi ke fact_sales\n",
    "df_fact = df_sales.groupby(['customerKey', 'productKey', 'territoryKey', 'timeKey']).agg(\n",
    "    totalQuantity=('orderqty', 'sum'),\n",
    "    averageAmount=('totaldue', 'mean'),\n",
    "    totalRevenue=('totaldue', 'sum')\n",
    ").reset_index()\n",
    "df_fact['salesID'] = range(1, len(df_fact) + 1)\n",
    "\n",
    "# Simpan ke staging.star_schema (sementara, nanti diload ke DW)\n",
    "df_customer.to_sql('dim_customer_incremental', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "df_product.to_sql('dim_product_incremental', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "df_territory.to_sql('dim_territory_incremental', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "df_time[['timeKey', 'year', 'month', 'day']].to_sql('dim_time_incremental', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "df_fact.to_sql('fact_sales_incremental', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "\n",
    "print(\"Incremental Extract & Transform selesai.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd9f56bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- STAGING: Copy AdventureWorks ke public schema -------------\n",
      "[CREATE] public.customer dibuat dan diisi.\n",
      "[CREATE] public.person dibuat dan diisi.\n",
      "[CREATE] public.product dibuat dan diisi.\n",
      "[CREATE] public.productcategory dibuat dan diisi.\n",
      "[CREATE] public.productsubcategory dibuat dan diisi.\n",
      "[CREATE] public.salesorderheader dibuat dan diisi.\n",
      "[CREATE] public.salesorderdetail dibuat dan diisi.\n",
      "[CREATE] public.salesterritory dibuat dan diisi.\n",
      "[CREATE] public.stateprovince dibuat dan diisi.\n",
      "[CREATE] public.countryregion dibuat dan diisi.\n",
      "✔️ Proses STAGING selesai. Semua tabel disalin ke schema 'public'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# ------------------- Koneksi Database -------------------\n",
    "hostname = \"localhost\"\n",
    "port = 5432\n",
    "username = \"postgres\"\n",
    "password = \"dataEngginer\"\n",
    "source_db = \"adventureworks\"\n",
    "staging_db = \"staggingDB\"\n",
    "\n",
    "# Engine koneksi\n",
    "source_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{source_db}\")\n",
    "staging_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{staging_db}\")\n",
    "\n",
    "# ------------------- FUNCTION STAGING TO PUBLIC -------------------\n",
    "def copy_to_staging(df, table_name, key_cols, engine, schema='public'):\n",
    "    with engine.connect() as conn:\n",
    "        try:\n",
    "            existing = pd.read_sql(f\"SELECT * FROM {schema}.{table_name}\", conn)\n",
    "        except:\n",
    "            # Jika tabel belum ada, buat baru\n",
    "            df.to_sql(table_name, engine, schema=schema, if_exists='replace', index=False)\n",
    "            print(f\"[CREATE] {schema}.{table_name} dibuat dan diisi.\")\n",
    "            return\n",
    "        # Bandingkan berdasarkan key\n",
    "        new_rows = df.merge(existing[key_cols], on=key_cols, how='left', indicator=True)\n",
    "        new_rows = new_rows[new_rows['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "\n",
    "        if not new_rows.empty:\n",
    "            # Contoh: tambahkan kolom jika dibutuhkan sebelum insert\n",
    "            if 'created_at' not in new_rows.columns:\n",
    "                new_rows['created_at'] = pd.Timestamp.now()\n",
    "\n",
    "            new_rows.to_sql(table_name, engine, schema=schema, if_exists='append', index=False)\n",
    "            print(f\"[APPEND] {len(new_rows)} baris baru ditambahkan ke {schema}.{table_name}.\")\n",
    "        else:\n",
    "            print(f\"[SKIP] Tidak ada data baru untuk {schema}.{table_name}.\")\n",
    "\n",
    "# ------------------- PROSES STAGING -------------------\n",
    "print(\"------------- STAGING: Copy AdventureWorks ke public schema -------------\")\n",
    "\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM sales.customer\", source_engine), 'customer', ['customerid'], staging_engine)\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM person.person\", source_engine), 'person', ['businessentityid'], staging_engine)\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM production.product\", source_engine), 'product', ['productid'], staging_engine)\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM production.productcategory\", source_engine), 'productcategory', ['productcategoryid'], staging_engine)\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM production.productsubcategory\", source_engine), 'productsubcategory', ['productsubcategoryid'], staging_engine)\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM sales.salesorderheader\", source_engine), 'salesorderheader', ['salesorderid'], staging_engine)\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM sales.salesorderdetail\", source_engine), 'salesorderdetail', ['salesorderid', 'productid'], staging_engine)\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM sales.salesterritory\", source_engine), 'salesterritory', ['territoryid'], staging_engine)\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM person.stateprovince\", source_engine), 'stateprovince', ['stateprovinceid'], staging_engine)\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM person.countryregion\", source_engine), 'countryregion', ['countryregioncode'], staging_engine)\n",
    "\n",
    "print(\"✔️ Proses STAGING selesai. Semua tabel disalin ke schema 'public'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca46a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- ETL: STAGING DATA dari AdventureWorks ke staggingDB (schema: public) -------------\n",
      "[SKIP] Tidak ada data baru untuk public.customer.\n",
      "[SKIP] Tidak ada data baru untuk public.person.\n",
      "[SKIP] Tidak ada data baru untuk public.product.\n",
      "[SKIP] Tidak ada data baru untuk public.productcategory.\n",
      "[SKIP] Tidak ada data baru untuk public.productsubcategory.\n",
      "[SKIP] Tidak ada data baru untuk public.salesorderheader.\n",
      "[SKIP] Tidak ada data baru untuk public.salesorderdetail.\n",
      "[SKIP] Tidak ada data baru untuk public.salesterritory.\n",
      "[SKIP] Tidak ada data baru untuk public.stateprovince.\n",
      "[SKIP] Tidak ada data baru untuk public.countryregion.\n",
      "✔️ ETL Staging selesai. Data berhasil disalin ke database staging (schema: public).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# ------------------- Koneksi Database -------------------\n",
    "hostname = \"localhost\"\n",
    "port = 5432\n",
    "username = \"postgres\"\n",
    "password = \"dataEngginer\"\n",
    "source_db = \"adventureworks\"\n",
    "staging_db = \"staggingDB\"\n",
    "\n",
    "# Engine untuk database sumber dan staging\n",
    "source_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{source_db}\")\n",
    "staging_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{staging_db}\")\n",
    "\n",
    "# ------------------- Fungsi ETL STAGING -------------------\n",
    "def copy_to_staging(df, table_name, key_cols, engine, schema='public'):\n",
    "    with engine.connect() as conn:\n",
    "        try:\n",
    "            existing = pd.read_sql(f\"SELECT * FROM {schema}.{table_name}\", conn)\n",
    "        except:\n",
    "            print(f\"[SKIP] Tabel {schema}.{table_name} belum ada. Silakan buat manual.\")\n",
    "            return\n",
    "        new_rows = df.merge(existing[key_cols], on=key_cols, how='left', indicator=True)\n",
    "        new_rows = new_rows[new_rows['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "        if not new_rows.empty:\n",
    "            new_rows.to_sql(table_name, engine, schema=schema, if_exists='append', index=False)\n",
    "            print(f\"[APPEND] {len(new_rows)} baris baru ditambahkan ke {schema}.{table_name}.\")\n",
    "        else:\n",
    "            print(f\"[SKIP] Tidak ada data baru untuk {schema}.{table_name}.\")\n",
    "\n",
    "# ------------------- LOAD RAW TABLES TO STAGING -------------------\n",
    "print(\"📥 ETL: Menyalin data mentah ke staging...\")\n",
    "\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM sales.customer\", source_engine), 'customer', ['customerid'], staging_engine)\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM person.person\", source_engine), 'person', ['businessentityid'], staging_engine)\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM production.product\", source_engine), 'product', ['productid'], staging_engine)\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM production.productcategory\", source_engine), 'productcategory', ['productcategoryid'], staging_engine)\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM production.productsubcategory\", source_engine), 'productsubcategory', ['productsubcategoryid'], staging_engine)\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM sales.salesorderheader\", source_engine), 'salesorderheader', ['salesorderid'], staging_engine)\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM sales.salesorderdetail\", source_engine), 'salesorderdetail', ['salesorderid', 'productid'], staging_engine)\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM sales.salesterritory\", source_engine), 'salesterritory', ['territoryid'], staging_engine)\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM person.stateprovince\", source_engine), 'stateprovince', ['stateprovinceid'], staging_engine)\n",
    "copy_to_staging(pd.read_sql(\"SELECT * FROM person.countryregion\", source_engine), 'countryregion', ['countryregioncode'], staging_engine)\n",
    "\n",
    "print(\"✅ ETL selesai: data mentah masuk ke staging.\")\n",
    "\n",
    "# ------------------- TRANSFORMASI RINGAN DI STAGING -------------------\n",
    "print(\"🔧 Mulai transformasi ringan...\")\n",
    "\n",
    "def transform_and_save(table_name, key_cols, transform_func=None):\n",
    "    with staging_engine.connect() as conn:\n",
    "        try:\n",
    "            df = pd.read_sql(f\"SELECT * FROM public.{table_name}\", conn)\n",
    "        except:\n",
    "            print(f\"[SKIP] Tabel public.{table_name} belum ditemukan.\")\n",
    "            return\n",
    "\n",
    "    df.columns = [col.lower() for col in df.columns]  # lowercase kolom\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    if transform_func:\n",
    "        df = transform_func(df)\n",
    "\n",
    "    # Simpan ke tabel *_clean\n",
    "    clean_name = f\"{table_name}_clean\"\n",
    "    df.to_sql(clean_name, staging_engine, schema='public', if_exists='replace', index=False)\n",
    "    print(f\"[CLEANED] Tabel {clean_name} berhasil disimpan.\")\n",
    "\n",
    "# ------------------- Contoh Fungsi Transformasi -------------------\n",
    "def transform_product(df):\n",
    "    if 'modifieddate' in df.columns:\n",
    "        df['modified_date'] = pd.to_datetime(df['modifieddate'])\n",
    "        df = df.drop(columns=['modifieddate'])\n",
    "    return df\n",
    "\n",
    "def transform_salesorderheader(df):\n",
    "    df['orderdate'] = pd.to_datetime(df['orderdate'])\n",
    "    df['duedate'] = pd.to_datetime(df['duedate'])\n",
    "    df['shipdate'] = pd.to_datetime(df['shipdate'])\n",
    "    return df\n",
    "\n",
    "# ------------------- Eksekusi Transformasi -------------------\n",
    "transform_and_save('product', ['productid'], transform_product)\n",
    "transform_and_save('customer', ['customerid'])\n",
    "transform_and_save('person', ['businessentityid'])\n",
    "transform_and_save('productcategory', ['productcategoryid'])\n",
    "transform_and_save('productsubcategory', ['productsubcategoryid'])\n",
    "transform_and_save('salesorderheader', ['salesorderid'], transform_salesorderheader)\n",
    "transform_and_save('salesorderdetail', ['salesorderid', 'productid'])\n",
    "transform_and_save('salesterritory', ['territoryid'])\n",
    "transform_and_save('stateprovince', ['stateprovinceid'])\n",
    "transform_and_save('countryregion', ['countryregioncode'])\n",
    "\n",
    "print(\"Transformasi ringan selesai. Semua tabel *_clean berhasil dibuat di staging.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
