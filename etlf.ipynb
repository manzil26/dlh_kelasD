{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "342e1b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous data found in DW.\n",
      "Incremental Extract & Transform selesai.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "\n",
    "# Koneksi database\n",
    "hostname = \"localhost\"\n",
    "port = 5432\n",
    "username = \"postgres\"\n",
    "password = \"dataEngginer\"\n",
    "source_db = \"adventureworks\"\n",
    "staging_db = \"staggingDB\"\n",
    "\n",
    "source_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{source_db}\")\n",
    "staging_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{staging_db}\")\n",
    "\n",
    "# ----------------- Dapatkan tanggal terakhir order dari data sebelumnya -----------------\n",
    "try:\n",
    "    last_df = pd.read_sql(\"SELECT MAX(orderdate) as last_date FROM star_schema.fact_sales\", staging_engine)\n",
    "    last_order_date = last_df['last_date'][0]\n",
    "    print(f\"Last order date in DW: {last_order_date}\")\n",
    "except:\n",
    "    last_order_date = None\n",
    "    print(\"No previous data found in DW.\")\n",
    "\n",
    "# ----------------- Extract data baru -----------------\n",
    "query_sales = \"\"\"\n",
    "SELECT soh.orderdate, soh.customerid, soh.territoryid, sod.productid,\n",
    "       sod.orderqty, soh.totaldue\n",
    "FROM sales.salesorderdetail sod\n",
    "JOIN sales.salesorderheader soh ON sod.salesorderid = soh.salesorderid\n",
    "{where_clause}\n",
    "\"\"\"\n",
    "where_clause = f\"WHERE soh.orderdate > '{last_order_date}'\" if last_order_date else \"\"\n",
    "df_sales = pd.read_sql(query_sales.format(where_clause=where_clause), source_engine)\n",
    "\n",
    "if df_sales.empty:\n",
    "    print(\"No new data to process.\")\n",
    "    exit()\n",
    "\n",
    "# ----------------- Buat dimensi -----------------\n",
    "\n",
    "# dim_customer\n",
    "query_customer = \"\"\"\n",
    "SELECT c.customerid,\n",
    "       pp.firstname || ' ' || COALESCE(pp.middlename || ' ', '') || pp.lastname AS customername\n",
    "FROM sales.customer c\n",
    "JOIN person.person pp ON c.personid = pp.businessentityid;\n",
    "\"\"\"\n",
    "df_customer = pd.read_sql(query_customer, source_engine)\n",
    "df_customer['customerKey'] = df_customer['customerid']\n",
    "\n",
    "# dim_product\n",
    "query_product = \"\"\"\n",
    "SELECT p.productid, pc.name AS productsubcategory, p.name AS productname\n",
    "FROM production.product p\n",
    "JOIN production.productsubcategory psc ON p.productsubcategoryid = psc.productsubcategoryid\n",
    "JOIN production.productcategory pc ON psc.productcategoryid = pc.productcategoryid;\n",
    "\"\"\"\n",
    "df_product = pd.read_sql(query_product, source_engine)\n",
    "df_product['productKey'] = df_product['productid']\n",
    "\n",
    "# dim_territory\n",
    "query_territory = \"\"\"\n",
    "SELECT st.territoryid, sp.name AS provincename, cr.name AS countryregion\n",
    "FROM sales.salesterritory st\n",
    "JOIN person.stateprovince sp ON st.territoryID = sp.territoryID\n",
    "JOIN person.countryregion cr ON st.countryregioncode = cr.countryregioncode;\n",
    "\"\"\"\n",
    "df_territory = pd.read_sql(query_territory, source_engine)\n",
    "df_territory['territoryKey'] = df_territory['territoryid']\n",
    "\n",
    "# dim_time\n",
    "df_time = df_sales[['orderdate']].drop_duplicates()\n",
    "df_time['year'] = pd.to_datetime(df_time['orderdate']).dt.year\n",
    "df_time['month'] = pd.to_datetime(df_time['orderdate']).dt.month\n",
    "df_time['day'] = pd.to_datetime(df_time['orderdate']).dt.day\n",
    "df_time['timeKey'] = pd.factorize(df_time['orderdate'])[0] + 1\n",
    "\n",
    "# Join surrogate keys\n",
    "df_sales = df_sales.merge(df_customer[['customerid', 'customerKey']], on='customerid')\n",
    "df_sales = df_sales.merge(df_product[['productid', 'productKey']], on='productid')\n",
    "df_sales = df_sales.merge(df_territory[['territoryid', 'territoryKey']], on='territoryid')\n",
    "df_sales = df_sales.merge(df_time[['orderdate', 'timeKey']], on='orderdate')\n",
    "\n",
    "# Agregasi ke fact_sales\n",
    "df_fact = df_sales.groupby(['customerKey', 'productKey', 'territoryKey', 'timeKey']).agg(\n",
    "    totalQuantity=('orderqty', 'sum'),\n",
    "    averageAmount=('totaldue', 'mean'),\n",
    "    totalRevenue=('totaldue', 'sum')\n",
    ").reset_index()\n",
    "df_fact['salesID'] = range(1, len(df_fact) + 1)\n",
    "\n",
    "# Simpan ke staging.star_schema (sementara, nanti diload ke DW)\n",
    "df_customer.to_sql('dim_customer_incremental', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "df_product.to_sql('dim_product_incremental', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "df_territory.to_sql('dim_territory_incremental', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "df_time[['timeKey', 'year', 'month', 'day']].to_sql('dim_time_incremental', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "df_fact.to_sql('fact_sales_incremental', staging_engine, schema='star_schema', if_exists='replace', index=False)\n",
    "\n",
    "print(\"Incremental Extract & Transform selesai.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84263e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_customer berhasil di-append.\n",
      "dim_product berhasil di-append.\n",
      "dim_territory berhasil di-append.\n",
      "dim_time berhasil di-append.\n",
      "Gagal memproses tabel fact_sales: (psycopg2.errors.UndefinedColumn) column \"orderdate\" of relation \"fact_sales\" does not exist\n",
      "LINE 1: ...\"totalQuantity\", \"averageAmount\", \"totalRevenue\", orderdate)...\n",
      "                                                             ^\n",
      "\n",
      "[SQL: INSERT INTO fact_sales (\"customerKey\", \"productKey\", \"territoryKey\", \"timeKey\", \"totalQuantity\", \"averageAmount\", \"totalRevenue\", orderdate) VALUES (%(customerKey__0)s, %(productKey__0)s, %(territoryKey__0)s, %(timeKey__0)s, %(totalQuantity__0)s, %(a ... 175916 characters truncated ... y__999)s, %(totalQuantity__999)s, %(averageAmount__999)s, %(totalRevenue__999)s, %(orderdate__999)s)]\n",
      "[parameters: {'totalRevenue__0': 24932.4138, 'averageAmount__0': 2770.2682, 'customerKey__0': 11000, 'orderdate__0': datetime.datetime(2013, 10, 3, 0, 0), 'productKey__0': 707, 'territoryKey__0': 9, 'totalQuantity__0': 9, 'timeKey__0': 854, 'totalRevenue__1': 33812.901, 'averageAmount__1': 3756.9889999999996, 'customerKey__1': 11000, 'orderdate__1': datetime.datetime(2011, 6, 21, 0, 0), 'productKey__1': 771, 'territoryKey__1': 9, 'totalQuantity__1': 9, 'timeKey__1': 22, 'totalRevenue__2': 23290.8921, 'averageAmount__2': 2587.8769, 'customerKey__2': 11000, 'orderdate__2': datetime.datetime(2013, 6, 20, 0, 0), 'productKey__2': 779, 'territoryKey__2': 9, 'totalQuantity__2': 9, 'timeKey__2': 749, 'totalRevenue__3': 23290.8921, 'averageAmount__3': 2587.8769, 'customerKey__3': 11000, 'orderdate__3': datetime.datetime(2013, 6, 20, 0, 0), 'productKey__3': 878, 'territoryKey__3': 9, 'totalQuantity__3': 9, 'timeKey__3': 749, 'totalRevenue__4': 24932.4138, 'averageAmount__4': 2770.2682, 'customerKey__4': 11000, 'orderdate__4': datetime.datetime(2013, 10, 3, 0, 0), 'productKey__4': 881, 'territoryKey__4': 9, 'totalQuantity__4': 9, 'timeKey__4': 854, 'totalRevenue__5': 24932.4138, 'averageAmount__5': 2770.2682, 'customerKey__5': 11000, 'orderdate__5': datetime.datetime(2013, 10, 3, 0, 0), 'productKey__5': 923, 'territoryKey__5': 9, 'totalQuantity__5': 9, 'timeKey__5': 854, 'totalRevenue__6': 24932.4138, 'averageAmount__6': 2770.2682 ... 7900 parameters truncated ... 'totalQuantity__993': 10, 'timeKey__993': 819, 'totalRevenue__994': 745.765, 'averageAmount__994': 74.5765, 'customerKey__994': 11153, 'orderdate__994': datetime.datetime(2013, 8, 29, 0, 0), 'productKey__994': 922, 'territoryKey__994': 1, 'totalQuantity__994': 10, 'timeKey__994': 819, 'totalRevenue__995': 467.194, 'averageAmount__995': 46.7194, 'customerKey__995': 11153, 'orderdate__995': datetime.datetime(2013, 8, 29, 0, 0), 'productKey__995': 930, 'territoryKey__995': 1, 'totalQuantity__995': 10, 'timeKey__995': 819, 'totalRevenue__996': 25635.89, 'averageAmount__996': 2563.589, 'customerKey__996': 11154, 'orderdate__996': datetime.datetime(2013, 6, 12, 0, 0), 'productKey__996': 780, 'territoryKey__996': 1, 'totalQuantity__996': 10, 'timeKey__996': 741, 'totalRevenue__997': 12817.945, 'averageAmount__997': 2563.589, 'customerKey__997': 11155, 'orderdate__997': datetime.datetime(2013, 6, 8, 0, 0), 'productKey__997': 779, 'territoryKey__997': 4, 'totalQuantity__997': 5, 'timeKey__997': 737, 'totalRevenue__998': 13204.5845, 'averageAmount__998': 2640.9169, 'customerKey__998': 11156, 'orderdate__998': datetime.datetime(2013, 6, 10, 0, 0), 'productKey__998': 782, 'territoryKey__998': 4, 'totalQuantity__998': 5, 'timeKey__998': 739, 'totalRevenue__999': 13204.5845, 'averageAmount__999': 2640.9169, 'customerKey__999': 11156, 'orderdate__999': datetime.datetime(2013, 6, 10, 0, 0), 'productKey__999': 880, 'territoryKey__999': 4, 'totalQuantity__999': 5, 'timeKey__999': 739}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "Incremental Load selesai.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "# Koneksi database\n",
    "hostname = \"localhost\"\n",
    "port = 5432\n",
    "username = \"postgres\"\n",
    "password = \"dataEngginer\"\n",
    "staging_db = \"staggingDB\"\n",
    "dw_db = \"adventureworksDw\"\n",
    "\n",
    "staging_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{staging_db}\")\n",
    "dw_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{dw_db}\")\n",
    "\n",
    "# Daftar tabel incremental\n",
    "incremental_tables = [\n",
    "    ('dim_customer_incremental', 'dim_customer'),\n",
    "    ('dim_product_incremental', 'dim_product'),\n",
    "    ('dim_territory_incremental', 'dim_territory'),\n",
    "    ('dim_time_incremental', 'dim_time'),\n",
    "    ('fact_sales_incremental', 'fact_sales')\n",
    "]\n",
    "\n",
    "for source_table, target_table in incremental_tables:\n",
    "    try:\n",
    "        df = pd.read_sql(f\"SELECT * FROM star_schema.{source_table}\", staging_engine)\n",
    "        if df.empty:\n",
    "            print(f\"Tidak ada data untuk {target_table}, skip.\")\n",
    "            continue\n",
    "        df.to_sql(target_table, dw_engine, if_exists='append', index=False)\n",
    "        print(f\"{target_table} berhasil di-append.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Gagal memproses tabel {target_table}: {e}\")\n",
    "\n",
    "print(\"Incremental Load selesai.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae3ef700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- Incremental Extract & Transform -------------------\n",
      "Gagal ambil max modifieddate dari DW: (psycopg2.errors.UndefinedTable) relation \"star_schema.dim_product\" does not exist\n",
      "LINE 1: SELECT MAX(modifieddate) FROM star_schema.dim_product\n",
      "                                      ^\n",
      "\n",
      "[SQL: SELECT MAX(modifieddate) FROM star_schema.dim_product]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "Last modified date in Staging: 2025-05-28 20:55:37.791273\n",
      "Jumlah data baru yang ditemukan: 0\n",
      "Tidak ada data baru yang perlu diproses.\n",
      "Incremental Extract & Transform selesai.\n"
     ]
    }
   ],
   "source": [
    "# incremental_etl_extract_transform.py\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------- Koneksi DB -------------------\n",
    "hostname = \"localhost\"\n",
    "port = 5432\n",
    "username = \"postgres\"\n",
    "password = \"dataEngginer\"\n",
    "source_db = \"adventureworks\"\n",
    "staging_db = \"staggingDB\"\n",
    "dw_db = \"adventureworksDw\"\n",
    "\n",
    "source_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{source_db}\")\n",
    "staging_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{staging_db}\")\n",
    "dw_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{dw_db}\")\n",
    "\n",
    "print(\"------------------- Incremental Extract & Transform -------------------\")\n",
    "\n",
    "# Ambil MAX(modifieddate) dari DW dan Staging\n",
    "try:\n",
    "    max_dw = pd.read_sql(\"SELECT MAX(modifieddate) FROM star_schema.dim_product\", dw_engine).iloc[0, 0]\n",
    "    if pd.isnull(max_dw):\n",
    "        max_dw = datetime(2000, 1, 1)\n",
    "    print(f\"Last modified date in DW: {max_dw}\")\n",
    "except Exception as e:\n",
    "    print(f\"Gagal ambil max modifieddate dari DW: {e}\")\n",
    "    max_dw = datetime(2000, 1, 1)\n",
    "\n",
    "try:\n",
    "    max_staging = pd.read_sql(\"SELECT MAX(modifieddate) FROM raw_schema.product\", staging_engine).iloc[0, 0]\n",
    "    if pd.isnull(max_staging):\n",
    "        max_staging = datetime(2000, 1, 1)\n",
    "    print(f\"Last modified date in Staging: {max_staging}\")\n",
    "except Exception as e:\n",
    "    print(f\"Gagal ambil max modifieddate dari Staging: {e}\")\n",
    "    max_staging = datetime(2000, 1, 1)\n",
    "\n",
    "# Ambil data baru dari OLTP\n",
    "query_incremental = f'''\n",
    "    SELECT p.productid, p.name AS productname, p.modifieddate,\n",
    "           psc.name AS productsubcategory, pc.name AS productcategory\n",
    "    FROM production.product p\n",
    "    LEFT JOIN production.productsubcategory psc ON p.productsubcategoryid = psc.productsubcategoryid\n",
    "    LEFT JOIN production.productcategory pc ON psc.productcategoryid = pc.productcategoryid\n",
    "    WHERE p.modifieddate > '{max(max_dw, max_staging)}'\n",
    "'''\n",
    "\n",
    "df_new_products = pd.read_sql(query_incremental, source_engine)\n",
    "print(f\"Jumlah data baru yang ditemukan: {len(df_new_products)}\")\n",
    "\n",
    "if not df_new_products.empty:\n",
    "    df_new_products['productKey'] = range(1, len(df_new_products) + 1)\n",
    "    df_new_products.to_sql('dim_product', staging_engine, schema='star_schema', if_exists='append', index=False)\n",
    "    print(\"Data baru berhasil ditransformasi dan disimpan ke staging star_schema.dim_product.\")\n",
    "else:\n",
    "    print(\"Tidak ada data baru yang perlu diproses.\")\n",
    "\n",
    "print(\"Incremental Extract & Transform selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61095381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- Incremental Load -------------------\n",
      "Gagal load ke DW: (psycopg2.errors.InvalidSchemaName) schema \"star_schema\" does not exist\n",
      "LINE 2: CREATE TABLE star_schema.dim_product (\n",
      "                     ^\n",
      "\n",
      "[SQL: \n",
      "CREATE TABLE star_schema.dim_product (\n",
      "\tproductid BIGINT, \n",
      "\tproductsubcategory TEXT, \n",
      "\tproductname TEXT, \n",
      "\t\"productKey\" BIGINT\n",
      ")\n",
      "\n",
      "]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "Incremental Load selesai.\n"
     ]
    }
   ],
   "source": [
    "# incremental_etl_load.py\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "# ------------------- Koneksi DB -------------------\n",
    "hostname = \"localhost\"\n",
    "port = 5432\n",
    "username = \"postgres\"\n",
    "password = \"dataEngginer\"\n",
    "dw_db = \"adventureworksDw\"\n",
    "staging_db = \"staggingDB\"\n",
    "\n",
    "dw_engine = create_engine(f'postgresql://{username}:{password}@{hostname}:{port}/{dw_db}')\n",
    "staging_engine = create_engine(f'postgresql://{username}:{password}@{hostname}:{port}/{staging_db}')\n",
    "\n",
    "print(\"------------------- Incremental Load -------------------\")\n",
    "\n",
    "try:\n",
    "    # Cek apakah tabel staging ada dan berisi data\n",
    "    df_staging = pd.read_sql(\"SELECT * FROM star_schema.dim_product\", staging_engine)\n",
    "    if df_staging.empty:\n",
    "        print(\"Staging star_schema.dim_product kosong. Tidak ada yang perlu diload.\")\n",
    "    else:\n",
    "        # Load ke DW\n",
    "        inspector = inspect(dw_engine)\n",
    "        if 'dim_product' in inspector.get_table_names(schema='star_schema'):\n",
    "            df_staging.to_sql('dim_product', dw_engine, schema='star_schema', if_exists='append', index=False)\n",
    "            print(f\"{len(df_staging)} baris data berhasil di-load ke DW star_schema.dim_product.\")\n",
    "        else:\n",
    "            df_staging.to_sql('dim_product', dw_engine, schema='star_schema', if_exists='fail', index=False)\n",
    "            print(\"Tabel dim_product baru berhasil dibuat dan data dimuat.\")\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"Gagal load ke DW: {e}\")\n",
    "\n",
    "print(\"Incremental Load selesai.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
